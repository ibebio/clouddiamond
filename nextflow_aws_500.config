params {

    // pipeline parameters
    aligner = "bowtie2"
    reference_db = "${projectDir}/data/ref/Col-CEN/"
    sra_accessions_list = "${projectDir}/data/demo/sra_acc_list_500.txt"


    // templates
    failed_run_report_template = "${projectDir}/assets/failed_sra_runs.template.html"

    // output
    outdir                     = './results'


}

//Indicate that we want to use awsbatch
process.executor = 'awsbatch'

//Indicate the name of the AWS Batch job queue that is configured in AWS Batch
process.queue = 'batch-align-queue'

//Name and version of the docker container we want to use within the Amazon ECS environment. //Users can optionally specify different docker containers for each process step
// process.container = ''

//The AWS region where we want to run
aws.region = 'us-east-1'

//Important note!!! If we create a custom AMI for use with AWS Batch
//we need to specify the path to where the AWS cli tool is preinstalled
aws.batch.cliPath = '/home/ec2-user/miniconda3/bin/aws'

//Additionally, if we want to use S3 to hold intermediate files we can specify an S3 bucket and work directory preconfigured in S3
workDir = 's3://aws-cloud-align-private/aws-cloud-align-test-20230918'

// Hopefully, this should give more memory upon retry


process {
  withLabel:process_low {
    cpus = {  2 * task.attempt }
    memory = {  7.GB * task.attempt }
    time = { 12.h * task.attempt }
  }
  withLabel:process_medium {
    cpus = { 2 * task.attempt }
    memory = { 7.GB * task.attempt }
    time = { 18.h * task.attempt }
  }
  withLabel:process_high {
    cpus = { 8 * task.attempt  }
    memory = { 14.GB * task.attempt }
    time = { 24.h * task.attempt }
  }

  errorStrategy = 'retry'
  maxRetries = 3
}


